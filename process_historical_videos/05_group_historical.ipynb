{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "from mpl_toolkits import mplot3d\n",
    "from shapely.geometry import Point, Polygon, MultiPoint, LineString\n",
    "from scipy.spatial import ConvexHull, convex_hull_plot_2d\n",
    "from itertools import combinations\n",
    "\n",
    "from datetime import datetime, timezone, tzinfo\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import networkx as nx\n",
    "import itertools\n",
    "from fiona.crs import from_epsg\n",
    "from sklearn.cluster import DBSCAN\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import networkx as nx\n",
    "\n",
    "from networkx import edge_betweenness_centrality as betweenness\n",
    "from networkx.algorithms.community.centrality import girvan_newman\n",
    "import json\n",
    "\n",
    "font = FontProperties()\n",
    "font.set_family('serif')\n",
    "font.set_name('Times New Roman')\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "sns.color_palette(\"hls\", 4)\n",
    "sns.set_context(\"paper\", rc={\"font.size\":10,\"axes.titlesize\":12,\"axes.labelsize\":12})\n",
    "from shapely.geometry import Point\n",
    "globalcrs = \"EPSG:3857\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_folder_ls = {\n",
    "    \"Bryant Park\":\"bryan_park_1980\",\n",
    "    \"Chestnut Street\":\"chestnut_street_1980\",\n",
    "    \"Downtown Crossing\":\"downtown_crossing_1980\",\n",
    "    \"MET\":\"met_1980\"\n",
    "}\n",
    "\n",
    "# load result path\n",
    "gcloudapi = \"AIzaSyCohhLdvyTC0UsGriQ9j-rU8pRln5wVVG8\"\n",
    "serviceaccount = \"/Users/yuan/Dropbox (Personal)/personal files/ssh/google_drive_personal.json\"\n",
    "import gspread\n",
    "# from oauth2client.service_account import ServiceAccountCredentials\n",
    "gc = gspread.service_account(filename = serviceaccount)\n",
    "\n",
    "\n",
    "def read_url(url, SHEET_NAME):\n",
    "    SHEET_ID = url.split('/')[5]\n",
    "    spreadsheet = gc.open_by_key(SHEET_ID)\n",
    "    worksheet = spreadsheet.worksheet(SHEET_NAME)\n",
    "    rows = worksheet.get_all_records()\n",
    "    df_spread = pd.DataFrame(rows)\n",
    "    return df_spread, worksheet\n",
    "\n",
    "url = \"https://docs.google.com/spreadsheets/d/1djLf9Uhh1zJpPBiSyjTnZ_EkkP1uZf2L8Rg8XWmXKlY/edit?usp=sharing\"\n",
    "SHEETNAME = \"P1_historical_videos\"\n",
    "video_path, other_worksheet = read_url( url, SHEETNAME)\n",
    "\n",
    "video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatecluster(df, dis, epsg):\n",
    "    \"\"\"\n",
    "    This function go through each frame and run DBscan based on different Distance\n",
    "    threshod\n",
    "\n",
    "    \"\"\"\n",
    "    predlist = []\n",
    "    for f in tqdm(df['frame_id'].unique()):\n",
    "        preDF = df[df['frame_id']==f].reset_index(drop = True)\n",
    "        X = preDF[[f\"x_{epsg}\", f\"y_{epsg}\"]].values\n",
    "\n",
    "        # eps : maximum distance between two samples\n",
    "        # Here we use pixel distance for ease of visualization\n",
    "        # 12 pixel for 0.5m, given hunman to human interaction distance maximum as 1.2 meter\n",
    "        # https://en.wikipedia.org/wiki/Proxemics#:~:text=Hall%20described%20the%20interpersonal%20distances,and%20(4)%20public%20space.\n",
    "\n",
    "        clustering = DBSCAN(eps = dis, min_samples = 2).fit(X)\n",
    "        pred = clustering.labels_\n",
    "        predlist.append(pred)\n",
    "    \n",
    "    allpred = np.concatenate(predlist, axis=0)\n",
    "    return allpred\n",
    "\n",
    "def getbasics(file_path):\n",
    "    video = cv2.VideoCapture(file_path)\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    length = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print('frames per second =',fps)\n",
    "    size = (int(video.get(cv2.CAP_PROP_FRAME_WIDTH)), int(video.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "    print('frames size =',size)\n",
    "    # video.release()\n",
    "    return video, fps, size, length\n",
    "\n",
    "def generate_social(traceGDF, epsg, dis = 1.5):\n",
    "    # dislist = [1.5, 2]\n",
    "    # distlistdict = {\n",
    "    #     2263: [1.2, 3.6], # feet\n",
    "    #     3857: [0.6, 1.5] # meter\n",
    "    # }\n",
    "    \"\"\"\n",
    "    d is the distance threshold for DBscan at meter\n",
    "    \"\"\"\n",
    "\n",
    "    clusterlabel = generatecluster(traceGDF, dis, epsg)\n",
    "    FPre = pd.DataFrame(clusterlabel, columns = ['Social'])\n",
    "    DBcluster = pd.concat([traceGDF, FPre], axis = 1)\n",
    "\n",
    "\n",
    "    DBcluster['frame_id'] = DBcluster['frame_id'].astype(int)\n",
    "    # Drop those clusterlabel == -1, create a spatial cluster id\n",
    "    DBcluster['group_id_social'] = DBcluster['frame_id'].astype(str) + '_' +DBcluster['Social'].astype(str)\n",
    "    DBcluster['group_id_social'] = np.where(DBcluster['Social']==-1, np.nan, DBcluster['group_id_social'])\n",
    "    DBSocial = DBcluster[DBcluster['Social']!=-1].reset_index(drop = True)\n",
    "    # os.makedirs(outfolder+\"/step1_dbscan\")\n",
    "    DBSocial['group_id_social'] = DBSocial['group_id_social'].astype(str)\n",
    "    return DBSocial, DBcluster\n",
    "\n",
    "\n",
    "\n",
    "def valid_link(DBSocial, x,y, thred = 0.5, n = 2):\n",
    "    samplegroup = DBSocial[DBSocial['track_id'].isin([x,y])]\n",
    "    # calculate the speed_x, speed_y correlation between track 10 and 11\n",
    "    df_wide = samplegroup.pivot(index = 'frame_id', \n",
    "                                columns = 'track_id', \n",
    "                                values = [f'speed_x_{n}s', \n",
    "                                          f'speed_y_{n}s',\n",
    "                                          f'speed_{n}s']).reset_index()\n",
    "    # calculate the correlation\n",
    "    df_wide = df_wide.dropna()\n",
    "    \n",
    "    coor1 = df_wide[(f\"speed_x_{n}s\",x)].corr(df_wide[(f\"speed_x_{n}s\",y)])\n",
    "    coor2 = df_wide[(f\"speed_y_{n}s\",x)].corr(df_wide[(f\"speed_y_{n}s\",y)])\n",
    "    coor3 = df_wide[(f\"speed_{n}s\",x)].corr(df_wide[(f\"speed_{n}s\",y)])\n",
    "    if coor1>thred and coor2>thred and coor3>thred:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def valid_link_corr(DBSocial, x,y, thred = 0.5, n = 2):\n",
    "    samplegroup = DBSocial[DBSocial['track_id'].isin([x,y])]\n",
    "    # calculate the speed_x, speed_y correlation between track 10 and 11\n",
    "    df_wide = samplegroup.pivot(index = 'frame_id', \n",
    "                                columns = 'track_id', \n",
    "                                values = [f'speed_x_{n}s', \n",
    "                                          f'speed_y_{n}s',\n",
    "                                          f'speed_{n}s']).reset_index()\n",
    "    # calculate the correlation\n",
    "    df_wide = df_wide.dropna()\n",
    "    \n",
    "    coor1 = df_wide[(f\"speed_x_{n}s\",x)].corr(df_wide[(f\"speed_x_{n}s\",y)])\n",
    "    coor2 = df_wide[(f\"speed_y_{n}s\",x)].corr(df_wide[(f\"speed_y_{n}s\",y)])\n",
    "    coor3 = df_wide[(f\"speed_{n}s\",x)].corr(df_wide[(f\"speed_{n}s\",y)])\n",
    "    return coor1, coor2, coor3\n",
    "    \n",
    "\n",
    "def getuvperframe(testdf, iditem):\n",
    "#     testdf = effDF[effDF['frame_id'] == frameid]\n",
    "    U = []\n",
    "    V = []\n",
    "    groupid = []\n",
    "    for i, group in tqdm(testdf.groupby([iditem])['track_id']):\n",
    "\n",
    "    # generate all combinations without replacement \n",
    "    # from the group of similar column pairs\n",
    "        for u, v in itertools.combinations(group, 2):\n",
    "            U.append(u)\n",
    "            V.append(v)\n",
    "            groupid.append(i)\n",
    "            \n",
    "    dfframe = pd.DataFrame({'u':U,\n",
    "             'v':V,\n",
    "              iditem: groupid\n",
    "             })\n",
    "\n",
    "    return dfframe\n",
    "\n",
    "\n",
    "# use girvan_newman for community detection first\n",
    "# df_plot = newlinks.reset_index(drop=True)\n",
    "\n",
    "def most_central_edge(G):\n",
    "    centrality = betweenness(G, weight=\"weight\")\n",
    "    return max(centrality, key=centrality.get)\n",
    "\n",
    "\n",
    "def getcommunity(df_links):\n",
    "    G_plot = nx.Graph()\n",
    "    for link in tqdm(df_links.index):\n",
    "        G_plot.add_edge(df_links.iloc[link]['u'],\n",
    "                    df_links.iloc[link]['v'],\n",
    "                    weight = df_links.iloc[link]['weight'])\n",
    "\n",
    "    communities = girvan_newman(G_plot, most_valuable_edge = most_central_edge)\n",
    "    # tuple(sorted(c) for c in next(comp))\n",
    "\n",
    "    node_groups = []\n",
    "    for com in next(communities):\n",
    "        node_groups.append(list(com))\n",
    "    node_groupslen = [len(group) for group in node_groups]\n",
    "    communitydf = pd.DataFrame({\n",
    "    'communityID':np.arange(0, len(node_groupslen)),\n",
    "    'nodegroup':node_groups,\n",
    "    'groupsize':node_groupslen\n",
    "    })\n",
    "    return communitydf\n",
    "\n",
    "def get_confirm_group(communitydf, DB, example_com):\n",
    "    \"\"\"This function only identify the exactly same group, disregarding groups appear across frames.\n",
    "    for example, track 1, 2 in frame 3-8, track 1,2,3 in frame 7-9\n",
    "    \"\"\"\n",
    "    tracks = communitydf[communitydf[\"communityID\"]==example_com][\"nodegroup\"].values[0]\n",
    "    tracks = [int(x) for x in tracks]\n",
    "    tracks.sort()\n",
    "    tracksls = \"_\".join([str(x) for x in tracks])\n",
    "\n",
    "    # for each frame and social id, get a list of tracks\n",
    "    sel = DB[(DB[\"communityID\"]==example_com)&(DB[\"track_id\"].isin(tracks))&(DB[\"Social\"]!=-1)]\n",
    "    frame_summary = sel.groupby([\"frame_id\", \"Social\"])[\"track_id\"].unique().reset_index()\n",
    "    frame_summary[\"track_id_str\"] = frame_summary[\"track_id\"].apply(lambda x: \"_\".join([str(i) for i in x]))\n",
    "    confirmed = frame_summary[frame_summary[\"track_id_str\"]==tracksls]\n",
    "    confirmed[\"communityID\"] = example_com\n",
    "    return confirmed\n",
    "\n",
    "\n",
    "def get_confirm_group_loose(communitydf, DB, example_com):\n",
    "    tracks = communitydf[communitydf[\"communityID\"]==example_com][\"nodegroup\"].values[0]\n",
    "    # tracks = [int(x) for x in tracks]\n",
    "    tracks.sort()\n",
    "    tracksls = \"_\".join([str(x) for x in tracks])\n",
    "\n",
    "    # for each frame and social id, get a list of tracks\n",
    "    sel = DB[(DB[\"communityID\"]==example_com)&(DB[\"track_id\"].isin(tracks))&(DB[\"Social\"]!=-1)]\n",
    "    frame_summary = sel.groupby([\"frame_id\", \"Social\"])[\"track_id\"].unique().reset_index()\n",
    "    frame_summary[\"track_id_str\"] = frame_summary[\"track_id\"].apply(lambda x: \"_\".join([str(i) for i in x]))\n",
    "    frame_summary[\"len\"] = frame_summary[\"track_id\"].apply(lambda x: len(x))\n",
    "    def get_inter(temp):\n",
    "        lst2 = temp[\"track_id_str\"].values[0].split(\"_\")\n",
    "        lst2 = [x for x in temp[\"track_id_str\"].values[0].split(\"_\")]\n",
    "        intersection = [value for value in lst2 if value in tracks]\n",
    "        temp[\"track_ls_intersection\"] = \"_\".join([str(x) for x in intersection])\n",
    "        return temp\n",
    "\n",
    "    reconstruct = frame_summary[frame_summary[\"len\"]>1].groupby(\"track_id_str\").apply(get_inter)\n",
    "    reconstruct[\"communityID\"] = example_com\n",
    "    return reconstruct\n",
    "\n",
    "\n",
    "def get_group(communitydf, DB):\n",
    "    group_df = []\n",
    "    for comID in tqdm(communitydf[\"communityID\"].unique()):\n",
    "        temp = get_confirm_group(communitydf, DB,comID)\n",
    "        group_df.append(temp)\n",
    "\n",
    "    group_df = pd.concat(group_df).reset_index(drop = True)\n",
    "    return group_df\n",
    "\n",
    "def get_group_loose(communitydf, DB):\n",
    "    group_df = []\n",
    "    for comID in tqdm(communitydf[\"communityID\"].unique()):\n",
    "        temp = get_confirm_group_loose(communitydf, DB, comID)\n",
    "        group_df.append(temp)\n",
    "\n",
    "    group_df = pd.concat(group_df).reset_index(drop = True)\n",
    "    return group_df\n",
    "\n",
    "def get_gender_comp(genderls):\n",
    "    if len(genderls)==2:\n",
    "        return \"Mixed\"\n",
    "    else:\n",
    "        return genderls[0]\n",
    "    \n",
    "# load prediction trace\n",
    "def get_pred(video_name):\n",
    "    pred_path = video_path[video_path['video_id'] == video_name]['pred_path'].values[0]\n",
    "\n",
    "    # ref = get_ref(ref_path)\n",
    "    trace = pd.read_csv(pred_path, sep = '\\t', header = None)\n",
    "    trace.columns = [ \"x1\", \"y1\", \"x2\", \"y2\", \"track_id\", \"frame_id\"]\n",
    "    trace['w'] = trace['x2'] - trace['x1']\n",
    "    trace['h'] = trace['y2'] - trace['y1']\n",
    "    trace['ratio'] = trace['w']/trace['h']\n",
    "    # trace.rename(columns = {\"x1\":\"bbox0\", \"y1\":\"bbox1\", \"w\":\"bbox2\", \"h\":\"bbox3\"}, inplace = True)\n",
    "    return trace\n",
    "\n",
    "def compute_color_for_labels(label):\n",
    "    \"\"\"\n",
    "    Simple function that adds fixed color depending on the class\n",
    "    \"\"\"\n",
    "    palette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)\n",
    "    color = [int((p * (label ** 2 - label + 1)) % 255) for p in palette]\n",
    "    return tuple(color)\n",
    "\n",
    "def valid_link_corr(DBSocial, x,y, thred = 0.5, n = 0.5):\n",
    "    samplegroup = DBSocial[DBSocial['track_id'].isin([x,y])]\n",
    "    # calculate the speed_x, speed_y correlation between track 10 and 11\n",
    "    df_wide = samplegroup.pivot(index = 'frame_id', \n",
    "                                columns = 'track_id', \n",
    "                                values = [f'speed_x_{n}s', \n",
    "                                          f'speed_y_{n}s',\n",
    "                                          f'speed_{n}s']).reset_index()\n",
    "    # calculate the correlation\n",
    "    df_wide = df_wide.dropna()\n",
    "    \n",
    "    coor1 = df_wide[(f\"speed_x_{n}s\",x)].corr(df_wide[(f\"speed_x_{n}s\",y)])\n",
    "    coor2 = df_wide[(f\"speed_y_{n}s\",x)].corr(df_wide[(f\"speed_y_{n}s\",y)])\n",
    "    coor3 = df_wide[(f\"speed_{n}s\",x)].corr(df_wide[(f\"speed_{n}s\",y)])\n",
    "    # corr4 is a stay indicator, if both speed_{n}s <0.5, thenn it is a stay\n",
    "    speed_mean1 = df_wide[(f\"speed_{n}s\",x)].mean()\n",
    "    speed_mean2 = df_wide[(f\"speed_{n}s\",y)].mean()\n",
    "    if speed_mean1<0.5 and speed_mean2<0.5:\n",
    "        coor4 = 1\n",
    "    else:\n",
    "        coor4 = 0\n",
    "    return coor1, coor2, coor3, coor4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load data\n",
    "Current data already contains speed vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# resultfolder_root = \"../../_data/05_tracking_result_projected/step0_attr_prj/\"\n",
    "staginging_folder = \"../../_data/05_tracking_result_projected/step1_speed_vector/historical\"\n",
    "finished = glob.glob(staginging_folder + \"/*_full.csv\")\n",
    "finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_loc = \"Chestnut Street\"\n",
    "\n",
    "# load one location videos\n",
    "filepath = os.path.join(staginging_folder, sample_loc+ \"_full.csv\")\n",
    "traceGDF = pd.read_csv(filepath)\n",
    "\n",
    "# note that across video names there are same track_id, so we need to add video name as prefix\n",
    "obs = traceGDF[['track_id','video_id']].drop_duplicates().shape[0]\n",
    "obs_wspeed = traceGDF[['track_id','video_id','speed_0.5s']].dropna(subset=['speed_0.5s']).drop_duplicates(['track_id','video_id']).shape[0]\n",
    "print(f\"Total number of observation in video {sample_loc}: \", obs, \"with speed vector: \", obs_wspeed)\n",
    "\n",
    "# drop the frame_id columns and rename frame_id_new to frame_id\n",
    "traceGDF.rename(columns = {'frame_id':'frame_id_original'}, inplace = True)\n",
    "traceGDF.rename(columns = {'frame_id_new':'frame_id'}, inplace = True)\n",
    "\n",
    "interval = 48 # this represents real world 1 second before sampling. The enlarged video has 480 per second. The video is timeplased 10 times. Therefore 48 frames per real-world second\n",
    "traceGDF['appear_sec'] = traceGDF.groupby(['track_id','video_id'])['frame_id'].transform('count')/interval\n",
    "originsize = traceGDF.shape[0]\n",
    "# # dropping people staying less than 3 seconds, for bryant park, we cannot do this given many people only appeared one time.\n",
    "# # set the threshold to be 0.2 second\n",
    "thred = 0.2*48\n",
    "traceGDF['individual_frame_total'] = traceGDF.groupby(['track_id','video_id'])['frame_id'].transform('nunique')\n",
    "keepGDF = traceGDF[traceGDF['individual_frame_total']>thred].reset_index(drop = True)\n",
    "keepsize = keepGDF.shape[0]\n",
    "per = keepsize/originsize\n",
    "print(\"KEEP {}% of the data\".format(per*100))\n",
    "\n",
    "# data resample threshold\n",
    "thre = 1\n",
    "fps = 479.97\n",
    "fps_adjust = fps/thre\n",
    "traceGDF = keepGDF[keepGDF['frame_id']%thre==0].reset_index(drop = True)\n",
    "traceGDF.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create Social Clusters at spatial level\n",
    "One video_id per calculation to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vizdatafolder =\"../../_data/05_demo/03_group_behavior_sample\"\n",
    "linkfolder = \"../../_data/05_tracking_result_projected/step2_graphs\"\n",
    "videols = traceGDF['video_id'].unique()\n",
    "len(videols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    'order':\"video order in one location\", \n",
    "     'video_location':\"video location name\", \n",
    "     'track_id':\"reconstructed track id, unique within each video\", \n",
    "       'video_id':\"video id, unique within each location\",\n",
    "       'lat':\"prejected latitude\",\n",
    "       'lon':\"prejected longitude\",\n",
    "       'track_id_backup':\"original track id from the tracking file\", \n",
    "       'speed_0.5s':\"speed in meter per second\",\n",
    "       'speed_x_0.5s':\"speed in meter per second in x direction\", \n",
    "       'speed_y_0.5s':\"speed in meter per second in y direction\", \n",
    "       'hex_id':\"h3 level 15 index\", \n",
    "       'inside':\"inside the comparable area (both historical and current) or not\",\n",
    "        'frame_id':\"reconstructed frame_id, across videos in a location, unique within one location\", \n",
    "        'frame_id_original':\"original frame_id from the tracking file\", \n",
    "        'second_from_start':\"calculated second from start based on the frame_id, 48 frames per real second\",\n",
    "       'appear_sec':\"total second the track appeared in the video\", \n",
    "       'individual_frame_total':\"total number of frames the track appeared in the video\", \n",
    "       'Social':\"spatial cluster id, unique within each frame, disregarding invalid or valid across time\", \n",
    "       'frame_social_track':\"frame_id + Social + track_id\",\n",
    "       'group_id_social':\"frame_id + Social, unique within each video\",\n",
    "       'group_size':\"number of tracks in the group\",\n",
    "       'is_group':\"whether the track is in a group or not\",\n",
    "       'group_first_frame':\"first frame_id when the track is in a group\",\n",
    "       'track_first_frame':\"first frame_id when the track appear in this video\", \n",
    "       'group_track_delta':\"difference between group_first_frame and track_first_frame\", \n",
    "       'emerging_group':\"whether the group is newly formed or not\",\n",
    "       'cross_frame_group_id':\\\n",
    "         \"this is a group id that can be used to identify the group across frames (only available for current videos)\",\n",
    "         \"gender\":\"gender of each pedestrian\",\n",
    "         \"age\":\"age of each pedestrian\",\n",
    "         'timestamp':\"timestamp of each frame (Only available for modern videos). use for reference.\"\n",
    "}\n",
    "selcols = list(metadata.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsg = 3857\n",
    "i = 2\n",
    "\n",
    "video_name = videols[i]\n",
    "\n",
    "tracecurrent = traceGDF[traceGDF['video_id']==video_name].reset_index(drop = True)\n",
    "tracecurrent = tracecurrent.drop_duplicates(['frame_id', 'track_id']).reset_index(drop = True)\n",
    "DBSocial, DBcluster =  generate_social(tracecurrent, epsg, dis = 1.9)\n",
    "\n",
    "# two person appear together for at least 1 second (60*2 frame per second )\n",
    "# or half of the appearing time\n",
    "selp = 'social'\n",
    "# selp = 'personal_far'\n",
    "iditem = 'group_id_{}'.format(selp)\n",
    "df_links = getuvperframe(DBSocial, iditem)\n",
    "arbithred = 24\n",
    "\n",
    "df_links = df_links.groupby(['u', 'v']).size().reset_index().rename(columns={0: 'weight'})\n",
    "\n",
    "print(df_links.shape[0], \"- First stage valid links\")\n",
    "df_links = df_links[(df_links['weight'] > arbithred)].reset_index(drop = True)\n",
    "df_links['coor_ls'] = df_links.apply(lambda x: valid_link_corr(DBSocial, x['u'],\n",
    "                                                               x['v'], n = 0.5), axis = 1)\n",
    "df_links['valid'] = np.where(df_links['coor_ls'].apply(lambda x: x[0]>0.0 or x[1]>0.0), True, False)\n",
    "df_links['valid'] = np.where(df_links['coor_ls'].apply(lambda x: x[3]>0), True, df_links['valid'])\n",
    "\n",
    "df_links_valid = df_links[(df_links['valid']==True)].reset_index(drop = True)\n",
    "\n",
    "print(df_links_valid.shape[0], \"- New valid links\")\n",
    "df_links_valid['u_v'] = df_links_valid.apply(lambda x: \"&&\".join([str(x['u']), str(x['v'])]), axis = 1)\n",
    "df_links_valid['v_u'] = df_links_valid.apply(lambda x: \"&&\".join([str(x['v']), str(x['u'])]), axis = 1)\n",
    "\n",
    "data_link = DBSocial.groupby(['frame_id', 'frame_id_original', 'Social'])['track_id'].unique().reset_index()\n",
    "data_link['group_member'] = data_link.apply(lambda x: \"&&\".join([str(i) for i in x['track_id']]), axis = 1)\n",
    "\n",
    "#measure group length\n",
    "data_link['group_len'] = data_link['track_id'].apply(lambda x: len(x))\n",
    "\n",
    "\n",
    "# make a list of all possible combination of 2 people\n",
    "data_link['combination2'] = data_link['track_id'].apply(lambda x: list(combinations(x, 2)))\n",
    "data_link_explode = data_link[['frame_id', 'frame_id_original', 'Social',\n",
    "       'combination2']].explode('combination2').reset_index(drop = True)\n",
    "data_link_explode['u_v'] = data_link_explode['combination2'].apply(lambda x: \"&&\".join([str(i) for i in x]))\n",
    "data_link_explode['v_u'] = data_link_explode['combination2'].apply(lambda x: \"&&\".join([str(i) for i in x[::-1]]))\n",
    "\n",
    "\n",
    "# check if the u_v combination exist in the valid link dataframe\n",
    "demolinks = data_link_explode[(data_link_explode['u_v'].isin(df_links_valid['u_v'].unique()))|(data_link_explode['u_v']\\\n",
    "    .isin(df_links_valid['v_u'].unique()))\n",
    "    ].reset_index(drop = True)\n",
    "\n",
    "\n",
    "# flatten the groups to each track\n",
    "demolinks = demolinks[['frame_id', \n",
    "                       'frame_id_original', \n",
    "                       'Social', 'combination2']].explode('combination2')\\\n",
    ".reset_index(drop = True).sort_values(['frame_id', 'Social'], ascending=False).rename(columns = {'combination2':'track_id'})\\\n",
    "    .drop_duplicates(['frame_id', 'Social', 'track_id']).reset_index(drop = True)\n",
    "demolinks['frame_social_track'] = demolinks['frame_id'].astype(str)+\"$$\"+\\\n",
    "    demolinks['Social'].astype(str)+\"$$\"+demolinks['track_id'].astype(str)\n",
    "\n",
    "# confirm the data can be merged back to the original data\n",
    "DBSocial['frame_social_track'] = DBSocial['frame_id'].astype(str)+ \"$$\"\\\n",
    "    + DBSocial['Social'].astype(str)+\"$$\"\\\n",
    "        +DBSocial['track_id'].astype(str)\n",
    "DBSocial_update = DBSocial[DBSocial['frame_social_track'].isin(demolinks['frame_social_track'].unique())]\\\n",
    "    .reset_index(drop = True)\n",
    "per = DBSocial_update.shape[0]/DBcluster.shape[0] # 26% observation ever in a group\n",
    "print(\"percentage of people ever in a group: \", per)\n",
    "\n",
    "# Social cluster id become the group id within each frame\n",
    "DBSocial_update['group_id_social'] = DBSocial_update['frame_id'].astype(str) + '_' +DBSocial_update['Social'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "# del fullgdf, traceGDF, DBSocial, DBcluster, df_links_valid, df_links, DBcluster_update\n",
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check emergeing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the True Group ID\n",
    "DBSocial_group = DBSocial_update.groupby(['frame_id', 'Social'])['track_id'].unique().reset_index()\n",
    "DBSocial_group['truegroup'] = DBSocial_group['track_id'].apply(lambda x: \"&&\".join([str(i) for i in x]))\n",
    "\n",
    "DBSocial_update = DBSocial_update[[ 'group_id_social','frame_id', 'Social','track_id']]\\\n",
    "                                    .merge(DBSocial_group[['frame_id', 'Social', 'truegroup']], \n",
    "                                            on = ['frame_id', 'Social'], how = 'inner')\n",
    "# for each track, if the frame_id within its group first and last frame, then it is a group\n",
    "DBsocial_group_update = []\n",
    "for truegroup in DBSocial_group['truegroup'].unique():\n",
    "    temp = DBSocial_update[DBSocial_update['truegroup']==truegroup].reset_index(drop = True)\n",
    "    trackls = temp['track_id'].unique()\n",
    "    firstframe = temp['frame_id'].min()\n",
    "    lastframe = temp['frame_id'].max()\n",
    "    # print(temp.shape[0])}\n",
    "    allvalid = DBcluster[(DBcluster['track_id'].isin(trackls))&(DBcluster['frame_id']<=lastframe)&(DBcluster['frame_id']>=firstframe)]\n",
    "    allvalid['truegroup'] = truegroup\n",
    "    # print(allvalid.shape[0])\n",
    "    DBsocial_group_update.append(allvalid)\n",
    "    gc.collect()\n",
    "DBsocial_group_update = pd.concat(DBsocial_group_update).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW WE MAY HAVE DUPLICATES>> MERGE THEM >> EACH TRACK SHOULD ONLY HAVE ONE TRUEGROUP IN ONE FRAME\n",
    "\n",
    "# DBsocial_group_update = DBSocial_update.copy()\n",
    "DBsocial_group_update['group_id_social'] = DBsocial_group_update['frame_id'].astype(str) + '_' +DBsocial_group_update['Social'].astype(str)\n",
    "DBsocial_group_update['frame_social_track'] = DBsocial_group_update['frame_id'].astype(str)+ \"$$\"\\\n",
    "    +DBsocial_group_update['Social'].astype(str)+\"$$\"\\\n",
    "        +DBsocial_group_update['track_id'].astype(str)\n",
    "DBsocial_group_update = DBsocial_group_update.drop_duplicates(['frame_id', 'track_id'])\n",
    "\n",
    "DBcluster.drop('group_id_social', axis = 1, inplace = True)\n",
    "\n",
    "DBcluster['frame_social_track'] = DBcluster['frame_id'].astype(str)+ \"$$\"\\\n",
    "    +DBcluster['Social'].astype(str)+\"$$\"\\\n",
    "        +DBcluster['track_id'].astype(str)\n",
    "# DBcluster.drop('group_id_social', axis = 1, inplace = True)\n",
    "DBcluster_update = DBcluster.merge(DBsocial_group_update[['frame_social_track', 'truegroup','group_id_social']],\n",
    "                                        on = 'frame_social_track', how = 'left')\n",
    "# merge the DBSocial_update back to the DBcluster\n",
    "DBcluster_update['is_group'] = np.where(DBcluster_update['group_id_social'].isnull(), False, True)\n",
    "\n",
    "# check if a group is newly formed or not\n",
    "# for each track, find its first frame_id when it is in a group\n",
    "DBcluster_update['group_first_frame'] = DBcluster_update.groupby(['track_id','is_group'])['frame_id'].transform('min')\n",
    "DBcluster_update['group_last_frame'] = DBcluster_update.groupby(['track_id','is_group'])['frame_id'].transform('max')\n",
    "DBcluster_update['group_first_frame'] = np.where(DBcluster_update['is_group']==False, \n",
    "                                                np.nan, DBcluster_update['group_first_frame'])\n",
    "DBcluster_update['group_last_frame'] = np.where(DBcluster_update['is_group']==False, \n",
    "                                                np.nan, DBcluster_update['group_last_frame'])\n",
    "\n",
    "\n",
    "DBcluster_update['track_first_frame'] = DBcluster_update.groupby(['track_id'])['frame_id'].transform('min')\n",
    "DBcluster_update['group_track_delta'] = DBcluster_update['group_first_frame'] - DBcluster_update['track_first_frame']\n",
    "DBcluster_update['emerging_group'] = np.where(DBcluster_update['group_track_delta']>29.97*5, True, False) \n",
    "DBcluster_update = DBcluster_update.drop_duplicates(['track_id',  'frame_id'])\n",
    "\n",
    "DBcluster_update['appear_sec'] = DBcluster_update.groupby('track_id')['frame_id'].transform('count')/fps\n",
    "DBcluster_update['individual_frame_total'] = DBcluster_update.groupby('track_id')['frame_id'].transform('count')\n",
    "DBcluster_update['group_size'] = DBcluster_update['truegroup'].fillna(\"\").apply(lambda x: len(x.split('&&')))\n",
    "DBcluster_update.rename(columns = {\n",
    "        'truegroup':'cross_frame_group_id',\n",
    "        \n",
    "    }, inplace = True)\n",
    "exportcols = [x for x in selcols if x in DBcluster_update.columns]\n",
    "assert DBcluster_update.shape[0]==tracecurrent.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per = DBsocial_group_update.shape[0]/DBcluster[DBcluster['individual_frame_total']>48].shape[0] # 26% observation ever in a group\n",
    "per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultfolder = '../../_data/10_clean/03_individual/historical/20230711d'\n",
    "if not os.path.exists(resultfolder):\n",
    "    os.makedirs(resultfolder)\n",
    "DBcluster_update[exportcols].to_csv(os.path.join(resultfolder,\n",
    "                                                 f\"{video_name}.csv\"), \n",
    "                                    index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed tests\n",
    "fig, ax = plt.subplots(figsize = (5,5))\n",
    "sns.pointplot(\n",
    "    data = DBcluster_update[DBcluster_update[\"speed_0.5s\"]<2.5],\n",
    "    x = \"group_size\",\n",
    "    y = \"speed_0.5s\",\n",
    "    ci = 98,\n",
    "    errwidth = 1,\n",
    "    capsize=.2,\n",
    "    color = \"#3bc0cf\"\n",
    ")\n",
    "ax.set_ylabel(\"Moving Speed (m/s)\")\n",
    "ax.set_xlabel(\"Group Size\")\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed tests\n",
    "fig, ax = plt.subplots(figsize = (5,5))\n",
    "sns.pointplot(\n",
    "    data = DBcluster_update[DBcluster_update[\"speed_0.5s\"]<2.5],\n",
    "    x = \"is_group\",\n",
    "    y = \"speed_0.5s\",\n",
    "    ci = 98,\n",
    "    errwidth = 1,\n",
    "    capsize=.2,\n",
    "    color = \"#3bc0cf\"\n",
    ")\n",
    "ax.set_ylabel(\"Moving Speed (m/s)\")\n",
    "ax.set_xlabel(\"In Group or Not\")\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    'order':\"video order in one location\", \n",
    "     'video_location':\"video location name\", \n",
    "     'track_id':\"reconstructed track id, unique within each video\", \n",
    "       'video_id':\"video id, unique within each location\",\n",
    "       'lat':\"prejected latitude\",\n",
    "       'lon':\"prejected longitude\",\n",
    "       'track_id_backup':\"original track id from the tracking file\", \n",
    "       'speed_0.5s':\"speed in meter per second\",\n",
    "       'speed_x_0.5s':\"speed in meter per second in x direction\", \n",
    "       'speed_y_0.5s':\"speed in meter per second in y direction\", \n",
    "       'hex_id':\"h3 level 15 index\", \n",
    "       'inside':\"inside the comparable area (both historical and current) or not\",\n",
    "        'frame_id':\"reconstructed frame_id, across videos in a location, unique within one location\", \n",
    "        'frame_id_original':\"original frame_id from the tracking file\", \n",
    "        'second_from_start':\"calculated second from start based on the frame_id, 48 frames per real second\",\n",
    "       'appear_sec':\"total second the track appeared in the video\", \n",
    "       'individual_frame_total':\"total number of frames the track appeared in the video\", \n",
    "       'Social':\"spatial cluster id, unique within each frame\", \n",
    "       'frame_social_track':\"frame_id + Social + track_id\",\n",
    "       'group_id_social':\"frame_id + Social, unique within each video\",\n",
    "       'group_size':\"number of tracks in the group\",\n",
    "       'is_group':\"whether the track is in a group or not\",\n",
    "       'group_first_frame':\"first frame_id when the track is in a group\",\n",
    "       'track_first_frame':\"first frame_id when the track appear in this video\", \n",
    "       'group_track_delta':\"difference between group_first_frame and track_first_frame\", \n",
    "       'emerging_group':\"whether the group is newly formed or not\"\n",
    "}\n",
    "DBcluster_update[metadata.keys()].to_csv(os.path.join(exportfolder, sample_loc+\"_\"+ video_name + \"_full.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(metadata, open(os.path.join(exportfolder, \"metadata.json\"), 'w'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viz sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the spatial group on the ground and image space\n",
    "num_group_frame = DBSocial.groupby(['frame_id', 'frame_id_original'])['group_id_social']\\\n",
    "    .nunique().reset_index().sort_values('group_id_social', ascending = False).reset_index(drop = True)\n",
    "framsel = num_group_frame.at[0, 'frame_id_original']\n",
    "print(\"Selected frame: \", framsel)\n",
    "# check frame_id and its immediate next frame_id\n",
    "def get_selfile(framsel, thre=2):\n",
    "\n",
    "    seldb = DBcluster[DBcluster['frame_id_original']==framsel].reset_index(drop = True)\n",
    "    seldb = gpd.GeoDataFrame(seldb, geometry=[Point(x,y) for x,y in zip(seldb['lon'], seldb['lat'])], crs = f\"EPSG:4326\")\n",
    "    ax = seldb[seldb['Social']!=-1].plot(column = 'Social', legend = True, cmap = 'tab20', figsize = (8,8))\n",
    "    seldb[seldb['Social']==-1].plot(color = 'grey', ax = ax)\n",
    "    \n",
    "    seldb_shift = DBcluster[DBcluster['frame_id_original']==framsel+thre*2].reset_index(drop = True)\n",
    "    seldb_shift = gpd.GeoDataFrame(seldb_shift, geometry=[Point(x,y) for x,y in zip(seldb_shift['lon'], \n",
    "                                                                                    seldb_shift['lat'])], \n",
    "                                   crs = f\"EPSG:4326\")\n",
    "    ax = seldb_shift[seldb_shift['Social']!=-1].plot(column = 'Social', legend = True, cmap = 'tab20', figsize = (8,8))\n",
    "    seldb_shift[seldb_shift['Social']==-1].plot(color = 'grey', ax = ax)\n",
    "    return seldb, seldb_shift\n",
    "seldb, seldb_shift = get_selfile(framsel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "video_name = videols[0]\n",
    "video_file = video_path[video_path['video_id']==video_name]['video_path'].values[0]\n",
    "# load the original trace file\n",
    "trace = get_pred(video_name)\n",
    "def get_data_for_plot(trace, seldb, seldb_shift):\n",
    "    data = trace.merge(seldb[[\"track_id_backup\", \"frame_id_original\", 'Social','track_id']], \n",
    "                                               left_on = [\"track_id\", \"frame_id\"],\n",
    "                                               right_on = [\"track_id_backup\", \"frame_id_original\"],\n",
    "                                               suffixes=('', '_new'))\n",
    "    data = data[data['Social']!=-1].reset_index(drop = True)\n",
    "    data_shift = trace.merge(seldb_shift[[\"track_id_backup\", \"frame_id_original\", 'Social','track_id']], \n",
    "                                               left_on = [\"track_id\", \"frame_id\"],\n",
    "                                               right_on = [\"track_id_backup\", \"frame_id_original\"],\n",
    "                                               suffixes=('', '_new'))\n",
    "    data_shift = data_shift[data_shift['Social']!=-1].reset_index(drop = True)\n",
    "    return data, data_shift\n",
    "\n",
    "def plot_img(video_name, framsel, data):\n",
    "    video_file = video_path[video_path['video_id']==video_name]['video_path'].values[0]\n",
    "    \n",
    "    video, fps, size, length= getbasics(video_file)\n",
    "        # set video to the frame\n",
    "    video.set(cv2.CAP_PROP_POS_FRAMES, framsel)\n",
    "    re, frame = video.read()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    for j in range(data.shape[0]):\n",
    "        social_id = data.at[j, 'Social']\n",
    "        color = compute_color_for_labels(social_id)\n",
    "        cv2.rectangle(frame,\n",
    "                        (data.at[j,'x1'], \n",
    "                    data.at[j,'y1']), \n",
    "                    (data.at[j,'x2'], \n",
    "                    data.at[j,'y2']), \n",
    "                        color, 2)\n",
    "\n",
    "    fig = plt.subplots(figsize = (10,10))\n",
    "    plt.imshow(frame)\n",
    "    plt.axis('off')\n",
    "    \n",
    "data, data_shift = get_data_for_plot(trace, seldb, seldb_shift)\n",
    "plot_img(video_name, framsel, data)\n",
    "plot_img(video_name, framsel+10, data_shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_demolink(data):\n",
    "    \"\"\"HERE THE DATA ONLY CONTAIN ONE FRAME.\"\"\"\n",
    "    data_link = data.groupby('Social')['track_id_new'].unique().reset_index()\n",
    "    data_link['u_v'] = data_link.apply(lambda x: \"&&\".join([str(i) for i in x['track_id_new']]), axis = 1)\n",
    "    demolinks = data_link[(data_link['u_v'].isin(df_links_valid['u_v'].unique()))|(data_link['u_v'].isin(df_links_valid['v_u'].unique()))\n",
    "        ].reset_index(drop = True)\n",
    "    demolinks = demolinks[['Social', 'track_id_new']].explode('track_id_new').reset_index(drop = True)\n",
    "    demolinks['social_track'] = demolinks['Social'].astype(str)+\"$$\"+demolinks['track_id_new'].astype(str)\n",
    "    return demolinks\n",
    "demolinks = get_demolink(data)\n",
    "demolinks_shift = get_demolink(data_shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seldb['social_track'] = seldb['Social'].astype(str)+\"$$\"+seldb['track_id'].astype(str)\n",
    "seldb_shift['social_track'] = seldb_shift['Social'].astype(str)+\"$$\"+seldb_shift['track_id'].astype(str)\n",
    "updateseldb = seldb[seldb['social_track'].isin(demolinks['social_track'].unique())]\n",
    "updateseldb_shift = seldb_shift[seldb_shift['social_track'].isin(demolinks_shift['social_track'].unique())]\n",
    "\n",
    "data_update, data_shift_update = get_data_for_plot(trace, updateseldb, updateseldb_shift)\n",
    "plot_img(video_name, framsel, data_update)\n",
    "plot_img(video_name, framsel+thre*2, data_shift_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
